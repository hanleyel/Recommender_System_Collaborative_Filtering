{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender System with Scipy linalg svds SVD Model (Option 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elizabeth Hanley\n",
    "### Uniqname: hanleyel\n",
    "### Kaggle ID: hanleyel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import gzip\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse import csc_matrix\n",
    "from scipy.sparse.linalg import svds\n",
    "import sklearn\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import normalize\n",
    "import csv\n",
    "from sparsesvd import sparsesvd\n",
    "import math\n",
    "import operator\n",
    "import random\n",
    "import decimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Unzips gzip data.\n",
    "def unzip_json(filename):\n",
    "    \n",
    "    print('Unzipping json file...')\n",
    "    \n",
    "    unzipped_data = pd.read_json(gzip.open(filename), lines=True)\n",
    "    \n",
    "    return unzipped_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unzipping json file...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-2d86cd013b06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0munzip_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'reviews.training.json.gz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-bb614c3a1c03>\u001b[0m in \u001b[0;36munzip_json\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unzipping json file...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0munzipped_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0munzipped_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/json/json.py\u001b[0m in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, lines, chunksize, compression)\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0mkeep_default_dates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeep_default_dates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0mprecise_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprecise_float\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdate_unit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdate_unit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m         \u001b[0mlines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m     )\n\u001b[1;32m    418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/json/json.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filepath_or_buffer, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, lines, chunksize, compression)\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data_from_filepath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_preprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_preprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/json/json.py\u001b[0m in \u001b[0;36m_preprocess_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    475\u001b[0m         \"\"\"\n\u001b[1;32m    476\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'read'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'read'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStringIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/gzip.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    274\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0merrno\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEBADF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"read() on write-only GzipFile object\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/gzip.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    483\u001b[0m                                \"end-of-stream marker was reached\")\n\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 485\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_read_data\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0muncompress\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    486\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pos\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muncompress\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0muncompress\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/gzip.py\u001b[0m in \u001b[0;36m_add_read_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_add_read_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 490\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_crc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrc32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_crc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    491\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stream_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stream_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "unzip_json('reviews.training.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Outputs json training data as a Pandas dataframe.\n",
    "def json_to_df(file_name):\n",
    "\n",
    "    print('Converting json file to dataframe...')\n",
    "\n",
    "    try:\n",
    "        training_data = pd.read_json(file_name, lines=True)\n",
    "        return training_data\n",
    "    except:\n",
    "        print('Please try another file name.')\n",
    "\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting json file to dataframe...\n"
     ]
    }
   ],
   "source": [
    "training_df = json_to_df('reviews.training.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dev_df = json_to_df('reviews.dev.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Converts dataframe to csv.\n",
    "def convert_to_csv(dataframe, desired_filename):\n",
    "\n",
    "    print('Converting dataframe to csv: ' + desired_filename + '...')\n",
    "\n",
    "    try:\n",
    "        return dataframe.to_csv(desired_filename, index=False)\n",
    "    except:\n",
    "        print('Please try another dataframe or file name.')\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting dataframe to csv: reviews.training.shortened.csv...\n"
     ]
    }
   ],
   "source": [
    "# Training file to CSV\n",
    "convert_to_csv(training_df[['reviewerID', 'asin', 'overall']].head(90000), 'reviews.training.shortened.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Development file to CSV.\n",
    "# convert_to_csv(dev_df[['reviewerID', 'asin', 'overall']], 'reviews.dev.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Returns dictionaries with unique users and products as keys and unique ints as values.\n",
    "def create_user_product_dicts(filename):\n",
    "\n",
    "    print('Creating dictionaries from CSV for unique users and products...')\n",
    "\n",
    "    user_dict = {}\n",
    "    product_dict = {}\n",
    "    user_count = 0\n",
    "    product_count = 0\n",
    "\n",
    "    with open(filename, 'r') as train_file:\n",
    "        file_reader=csv.reader(train_file, delimiter=',')\n",
    "        next(file_reader, None)\n",
    "\n",
    "        for row in file_reader:\n",
    "            if row[0] not in user_dict:\n",
    "                user_dict[row[0]] = user_count\n",
    "                user_count += 1\n",
    "            if row[1] not in product_dict:\n",
    "                product_dict[row[1]] = product_count\n",
    "                product_count += 1\n",
    "\n",
    "    return user_dict, product_dict, user_count, product_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dictionaries from CSV for unique users and products...\n"
     ]
    }
   ],
   "source": [
    "user_dict, product_dict, user_count, product_count = create_user_product_dicts('reviews.training.shortened.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creates a dense matrix from training data.\n",
    "def training_mtx(filename, user_dict, product_dict):\n",
    "\n",
    "        print('Creating a dense matrix from training data...')\n",
    "\n",
    "        num_user_ids = len(user_dict)\n",
    "        num_product_ids = len(product_dict)\n",
    "\n",
    "        dense_matrix = np.zeros(shape=(num_user_ids, num_product_ids), dtype=np.float32)\n",
    "\n",
    "        with open(filename, 'r') as train_file:\n",
    "            matrix_reader = csv.reader(train_file, delimiter=',')\n",
    "            next(matrix_reader, None)\n",
    "            for row in matrix_reader:\n",
    "                dense_matrix[user_dict[row[0]], product_dict[row[1]]] = float(row[2])\n",
    "\n",
    "        return dense_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a dense matrix from training data...\n"
     ]
    }
   ],
   "source": [
    "training_matrix = training_mtx('reviews.training.shortened.csv', user_dict, product_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 5., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 5., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Outputs dictionaries with unique test users and test products.\n",
    "def get_test_users_products(filename, training_user_dict, training_product_dict):\n",
    "\n",
    "    print('Importing test users and products...')\n",
    "\n",
    "    test_user_count = len(training_user_dict)\n",
    "    test_product_count = len(training_product_dict)\n",
    "    test_user_dict = training_user_dict.copy()\n",
    "    test_product_dict = training_product_dict.copy()\n",
    "\n",
    "    with open(filename, 'r') as test_file:\n",
    "        test_reader = csv.reader(test_file, delimiter=',')\n",
    "        next(test_reader, None)\n",
    "\n",
    "        for row in test_reader:\n",
    "            # Add unique users to test_user dictionary.\n",
    "            if row[0] not in test_user_dict:\n",
    "                test_user_dict[row[0]] = test_user_count\n",
    "                test_user_count += 1\n",
    "            # Add unique products to test_product dictionary.\n",
    "            # print(row[2])\n",
    "            if row[1] not in test_product_dict:\n",
    "                test_product_dict[row[1]] = test_product_count\n",
    "                test_product_count += 1\n",
    "\n",
    "    return test_user_dict, test_product_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing test users and products...\n"
     ]
    }
   ],
   "source": [
    "test_user_dict, test_product_dict = get_test_users_products('reviews.dev.csv', user_dict, product_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59641"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_user_dict['A16NGP74HECTI9']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90455"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_user_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41821"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_product_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creates a new matrix with unknown products on the x axis.\n",
    "# def merged_mtx_products(filename, user_dict, test_product_dict):\n",
    "    \n",
    "#         print('Creating a matrix with new products on the x axis...')\n",
    "\n",
    "#         num_user_ids = len(user_dict)\n",
    "#         num_product_ids = len(test_product_dict)\n",
    "\n",
    "#         dense_matrix = np.zeros(shape=(num_user_ids, num_product_ids), dtype=np.float32)\n",
    "\n",
    "#         with open(filename, 'r') as train_file:\n",
    "#             matrix_reader = csv.reader(train_file, delimiter=',')\n",
    "#             next(matrix_reader, None)\n",
    "#             for row in matrix_reader:\n",
    "#                 dense_matrix[user_dict[row[0]], product_dict[row[1]]] = float(row[2])\n",
    "\n",
    "#         return dense_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_matrix_product_rows = merged_mtx_products('reviews.training.shortened.csv', user_dict, test_product_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_matrix_product_rows.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_matrix_product_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum(merged_matrix_product_rows[-1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_matrix_user_rows.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_matrix_user_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum(merged_matrix_user_rows[-1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Converts dense matrices to sparse.\n",
    "def to_sparse(filename_prefix, matrix):\n",
    "    print('Creating a sparse matrix...')\n",
    "    try:\n",
    "        # Try loading previously saved sparse matrix from file (becaues I keep crashing my kernel)\n",
    "        loader = np.load(filename_prefix + '.npz')\n",
    "        sparse_matrix = csr_matrix((loader['data'], loader['indices'], loader['indptr']), shape=loader['shape'], dtype=np.float32)\n",
    "        loader.close()\n",
    "    except:\n",
    "        # Create sparse matrix from dense matrix, write to file as backup\n",
    "        sparse_matrix = scipy.sparse.csr_matrix(matrix, dtype=np.float32)\n",
    "        scipy.sparse.save_npz((filename_prefix + 'npz'), sparse_matrix)\n",
    "    return sparse_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sparse_merged_matrix_pr = to_sparse('sparse.merged.matrix.pr', merged_matrix_product_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a sparse matrix...\n"
     ]
    }
   ],
   "source": [
    "sparse_matrix = to_sparse('merged.matrix', training_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49156, 27951)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sparse_merged_matrix_ur.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate global, row, and column means\n",
    "def calculate_means(sparse_matrix):\n",
    "    print('Calculating global mean...')\n",
    "    # global_mean = sparse_matrix.sum()/(sparse_matrix != 0).sum()\n",
    "    global_mean = np.true_divide(sparse_matrix.sum(), (sparse_matrix != 0).sum(), dtype=np.float32)\n",
    "    print(global_mean)\n",
    "    \n",
    "    print('Calculating row mean...')\n",
    "    matrix_row_mean = np.true_divide(sparse_matrix.sum(1), (sparse_matrix != 0).sum(1), dtype=np.float32)\n",
    "    print(matrix_row_mean[-1])\n",
    "    \n",
    "    np.savetxt(\"row.mean.csv\", matrix_row_mean, delimiter=\",\")\n",
    "    \n",
    "#     row_pad_len = len(test_user_dict) - len(user_dict)\n",
    "#     matrix_row_mean_padded = np.pad(matrix_row_mean, (0, row_pad_len), 'constant')\n",
    "#     print(matrix_row_mean_padded)\n",
    "    \n",
    "    \n",
    "    print('Calculating column mean...')\n",
    "    matrix_column_mean = np.true_divide(sparse_matrix.sum(0), (sparse_matrix != 0).sum(0), dtype=np.float32)\n",
    "    print(matrix_column_mean)\n",
    "    \n",
    "    np.savetxt(\"column.mean.csv\", matrix_column_mean.T, delimiter=\",\")\n",
    "\n",
    "    return global_mean, matrix_row_mean, matrix_column_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating global mean...\n",
      "4.1057\n",
      "Calculating row mean...\n",
      "[[4.]]\n",
      "Calculating column mean...\n",
      "[[3.6585367 3.8947368 5.        ... 5.        5.        5.       ]]\n"
     ]
    }
   ],
   "source": [
    "global_mean, matrix_row_mean, matrix_column_mean = calculate_means(sparse_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge test data and normalize matrix.\n",
    "def normalize_matrix(sparse_matrix, global_mean, matrix_row_mean, matrix_column_mean):\n",
    "    \n",
    "    print('Creating a new matrix for merging test data scores...')\n",
    "\n",
    "    num_product_ids = len(test_product_dict)\n",
    "    num_user_ids = len(test_user_dict)\n",
    "\n",
    "    dense_merged_matrix = np.full((num_user_ids+1, num_product_ids+1), 0, dtype=np.float32)\n",
    "    \n",
    "    \n",
    "    with open('row.mean.csv', 'r') as infile:\n",
    "        file_reader = csv.reader(infile, delimiter=',')\n",
    "        row_count = 0\n",
    "        for row in file_reader:\n",
    "            dense_merged_matrix[row_count, :] = (float(row[0])-global_mean)/2\n",
    "            row_count += 1\n",
    "            \n",
    "    print(dense_merged_matrix)\n",
    "            \n",
    "    with open('column.mean.csv', 'r') as infile:\n",
    "        file_reader = csv.reader(infile, delimiter=',')\n",
    "        column_count = 0\n",
    "        for row in file_reader:\n",
    "            dense_merged_matrix[:, column_count] += (float(row[0])-global_mean)/2\n",
    "            column_count += 1\n",
    "\n",
    "#     print('Normalizing the data...')\n",
    "#     normalized_matrix = np.add(merged_matrix, matrix_row_mean_padded)\n",
    "    \n",
    "#     print('Normalized matrix: ')\n",
    "#     print(normalized_matrix.shape)\n",
    "#     print(normalized_matrix)\n",
    "\n",
    "    return dense_merged_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a new matrix for merging test data scores...\n",
      "[[-0.05285001 -0.05285001 -0.05285001 ... -0.05285001 -0.05285001\n",
      "  -0.05285001]\n",
      " [-0.05285001 -0.05285001 -0.05285001 ... -0.05285001 -0.05285001\n",
      "  -0.05285001]\n",
      " [ 0.44715     0.44715     0.44715    ...  0.44715     0.44715\n",
      "   0.44715   ]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]]\n"
     ]
    }
   ],
   "source": [
    "normalized_matrix = normalize_matrix(sparse_matrix, global_mean, matrix_row_mean, matrix_column_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90456, 41822)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.27643168, -0.15833163,  0.39429998, ..., -0.05285001,\n",
       "        -0.05285001, -0.05285001],\n",
       "       [-0.27643168, -0.15833163,  0.39429998, ..., -0.05285001,\n",
       "        -0.05285001, -0.05285001],\n",
       "       [ 0.22356832,  0.34166837,  0.8943    , ...,  0.44715   ,\n",
       "         0.44715   ,  0.44715   ],\n",
       "       ...,\n",
       "       [-0.22358167, -0.10548162,  0.44715   , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.22358167, -0.10548162,  0.44715   , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.22358167, -0.10548162,  0.44715   , ...,  0.        ,\n",
       "         0.        ,  0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalized_matrix_users, matrix_row_mean_users, global_mean = normalize_merged_matrix(sparse_merged_matrix_ur, merged_matrix_user_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implements an SVD model.\n",
    "def compute_svd_from_normalized(normalized_matrix):\n",
    "\n",
    "    print('Computing svd from de-meaned matrix...')\n",
    "\n",
    "    U, sigma, Vt = svds(normalized_matrix, k = 100)\n",
    "    # U, sigma, Vt = np.linalg.svd(normalized_matrix)\n",
    "    S = np.diag(sigma)\n",
    "\n",
    "    return U, S, Vt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, S, Vt = compute_svd_from_normalized(normalized_matrix.astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_predictions = np.dot(np.dot(U, S), Vt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions_matrix = normalized_predictions+global_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_product_dict['B003F3NE1Q']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def query_matrix(infile, outfile, predictions_matrix):\n",
    "    \n",
    "    print(predictions_matrix.shape)\n",
    "    \n",
    "    with open(infile, 'r') as test_file:\n",
    "        test_reader = csv.reader(test_file, delimiter=',')\n",
    "        next(test_reader, None)\n",
    "        with open(outfile, 'w') as outfile:\n",
    "            outfile_reader = csv.writer(outfile, delimiter=',')\n",
    "            outfile_reader.writerow(['datapointID', 'overall'])\n",
    "\n",
    "            for row in test_reader:\n",
    "                \n",
    "                try:\n",
    "                    prediction = predictions_matrix[test_user_dict[row[0]], test_product_dict[row[1]]]\n",
    "                    outfile_reader.writerow([row[0], row[2], prediction])\n",
    "                    print(row[2], prediction)\n",
    "                except:\n",
    "                    print('Error')\n",
    "                    pass\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_matrix('reviews.dev.csv', 'reviews.test.labeled.csv', predictions_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Queries the prediction matrix.\n",
    "def query_normalized_matrix_test(test_file, prediction_file, test_user_dict, test_product_dict,\n",
    "                                predictions_product_rows, predictions_user_rows, global_mean):\n",
    "\n",
    "    print('Reconstructing matrix and making predictions...')\n",
    "\n",
    "    with open(test_file, 'r') as test_file:\n",
    "        test_reader = csv.reader(test_file, delimiter=',')\n",
    "        next(test_reader, None)\n",
    "        with open(prediction_file, 'w') as outfile:\n",
    "            outfile_reader = csv.writer(outfile, delimiter=',')\n",
    "            outfile_reader.writerow(['datapointID', 'overall'])\n",
    "            \n",
    "\n",
    "\n",
    "            for row in test_reader:\n",
    "                prediction = random.randrange(1,5,1)\n",
    "\n",
    "                try:\n",
    "                    # Query by user (new products on x axis).\n",
    "                    user_query = predictions_product_rows[test_user_dict[row[1]]]\n",
    "                    prediction = user_query[0, test_product_dict[row[2]]]\n",
    "                    outfile_reader.writerow([row[0], prediction])\n",
    "                    # print('Query by USER')\n",
    "                    # print(prediction)\n",
    "                except:\n",
    "                    pass\n",
    "#                     try:\n",
    "#                         # Query by product.\n",
    "#                         product_query = predictions_user_rows[product_dict[row[2]]]\n",
    "#                         prediction = product_query[0, test_user_dict[row[1]]]\n",
    "#                         outfile_reader.writerow([row[0], prediction])\n",
    "#                         # print('Query by PRODUCT')\n",
    "#                         # print(prediction)\n",
    "#                     except:\n",
    "#                         # If no matching users or products are found, make prediction based on global mean.\n",
    "#                         prediction = global_mean\n",
    "#                         outfile_reader.writerow([row[0], prediction])\n",
    "#                         # print('No matching query: GLOBAL MEAN')\n",
    "#                         # print(prediction)\n",
    "\n",
    "    print('Done.')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_ratings = query_normalized_matrix_test('reviews.dev.csv', 'reviews.test.labeled.csv',\n",
    "                                                test_user_dict, test_product_dict, predictions_product_rows,\n",
    "                                                predictions_user_rows, global_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# RMSE analysis\n",
    "def rmse(analysis_file):\n",
    "    \n",
    "    print('Checking RMSE...')\n",
    "    \n",
    "    targets = np.array([])\n",
    "    predictions = np.array([])\n",
    "\n",
    "    with open(analysis_file, 'r') as analysis_file:\n",
    "        analysis_reader = csv.reader(analysis_file, delimiter=',')\n",
    "        next(analysis_reader, None)\n",
    "        for row in analysis_reader:\n",
    "                targets = np.append(targets, row[1])\n",
    "                predictions = np.append(predictions, row[2])\n",
    "\n",
    "    return np.sqrt(((predictions - targets) ** 2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# rmse_val = rmse('reviews.dev.labeled.2.csv')\n",
    "# print(\"rms error is: \" + str(rmse_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
