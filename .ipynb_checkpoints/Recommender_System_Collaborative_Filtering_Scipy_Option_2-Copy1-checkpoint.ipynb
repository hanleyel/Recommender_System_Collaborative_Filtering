{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender System with Scipy linalg svds SVD Model (Option 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elizabeth Hanley\n",
    "### Uniqname: hanleyel\n",
    "### Kaggle ID: hanleyel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import gzip\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse import csc_matrix\n",
    "from scipy.sparse.linalg import svds\n",
    "import sklearn\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import normalize\n",
    "import csv\n",
    "from sparsesvd import sparsesvd\n",
    "import math\n",
    "import operator\n",
    "import random\n",
    "import decimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Unzips gzip data.\n",
    "def unzip_json(filename):\n",
    "    \n",
    "    print('Unzipping json file...')\n",
    "    \n",
    "    unzipped_data = pd.read_json(gzip.open(filename), lines=True)\n",
    "    \n",
    "    return unzipped_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unzip_json('reviews.training.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Outputs json training data as a Pandas dataframe.\n",
    "def json_to_df(file_name):\n",
    "\n",
    "    print('Converting json file to dataframe...')\n",
    "\n",
    "    try:\n",
    "        training_data = pd.read_json(file_name, lines=True)\n",
    "        return training_data\n",
    "    except:\n",
    "        print('Please try another file name.')\n",
    "\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df = json_to_df('reviews.training.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dev_df = json_to_df('reviews.dev.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Converts dataframe to csv.\n",
    "def convert_to_csv(dataframe, desired_filename):\n",
    "\n",
    "    print('Converting dataframe to csv: ' + desired_filename + '...')\n",
    "\n",
    "    try:\n",
    "        return dataframe.to_csv(desired_filename, index=False)\n",
    "    except:\n",
    "        print('Please try another dataframe or file name.')\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training file to CSV\n",
    "convert_to_csv(training_df[['reviewerID', 'asin', 'overall']].head(90000), 'reviews.training.shortened.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Development file to CSV.\n",
    "# convert_to_csv(dev_df[['reviewerID', 'asin', 'overall']], 'reviews.dev.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Returns dictionaries with unique users and products as keys and unique ints as values.\n",
    "def create_user_product_dicts(filename):\n",
    "\n",
    "    print('Creating dictionaries from CSV for unique users and products...')\n",
    "\n",
    "    user_dict = {}\n",
    "    product_dict = {}\n",
    "    user_count = 0\n",
    "    product_count = 0\n",
    "\n",
    "    with open(filename, 'r') as train_file:\n",
    "        file_reader=csv.reader(train_file, delimiter=',')\n",
    "        next(file_reader, None)\n",
    "\n",
    "        for row in file_reader:\n",
    "            if row[0] not in user_dict:\n",
    "                user_dict[row[0]] = user_count\n",
    "                user_count += 1\n",
    "            if row[1] not in product_dict:\n",
    "                product_dict[row[1]] = product_count\n",
    "                product_count += 1\n",
    "\n",
    "    return user_dict, product_dict, user_count, product_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_dict, product_dict, user_count, product_count = create_user_product_dicts('reviews.training.shortened.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creates a dense matrix from training data.\n",
    "def training_mtx(filename, user_dict, product_dict):\n",
    "\n",
    "        print('Creating a dense matrix from training data...')\n",
    "\n",
    "        num_user_ids = len(user_dict)\n",
    "        num_product_ids = len(product_dict)\n",
    "\n",
    "        dense_matrix = np.zeros(shape=(num_user_ids, num_product_ids), dtype=np.float32)\n",
    "\n",
    "        with open(filename, 'r') as train_file:\n",
    "            matrix_reader = csv.reader(train_file, delimiter=',')\n",
    "            next(matrix_reader, None)\n",
    "            for row in matrix_reader:\n",
    "                dense_matrix[user_dict[row[0]], product_dict[row[1]]] = float(row[2])\n",
    "\n",
    "        return dense_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_matrix = training_mtx('reviews.training.shortened.csv', user_dict, product_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Outputs dictionaries with unique test users and test products.\n",
    "def get_test_users_products(filename, training_user_dict, training_product_dict):\n",
    "\n",
    "    print('Importing test users and products...')\n",
    "\n",
    "    test_user_count = len(training_user_dict)\n",
    "    test_product_count = len(training_product_dict)\n",
    "    test_user_dict = training_user_dict.copy()\n",
    "    test_product_dict = training_product_dict.copy()\n",
    "\n",
    "    with open(filename, 'r') as test_file:\n",
    "        test_reader = csv.reader(test_file, delimiter=',')\n",
    "        next(test_reader, None)\n",
    "\n",
    "        for row in test_reader:\n",
    "            # Add unique users to test_user dictionary.\n",
    "            if row[0] not in test_user_dict:\n",
    "                test_user_dict[row[0]] = test_user_count\n",
    "                test_user_count += 1\n",
    "            # Add unique products to test_product dictionary.\n",
    "            # print(row[2])\n",
    "            if row[1] not in test_product_dict:\n",
    "                test_product_dict[row[1]] = test_product_count\n",
    "                test_product_count += 1\n",
    "\n",
    "    return test_user_dict, test_product_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_user_dict, test_product_dict = get_test_users_products('reviews.dev.csv', user_dict, product_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_user_dict['A16NGP74HECTI9']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_user_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_product_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creates a new matrix with unknown products on the x axis.\n",
    "# def merged_mtx_products(filename, user_dict, test_product_dict):\n",
    "    \n",
    "#         print('Creating a matrix with new products on the x axis...')\n",
    "\n",
    "#         num_user_ids = len(user_dict)\n",
    "#         num_product_ids = len(test_product_dict)\n",
    "\n",
    "#         dense_matrix = np.zeros(shape=(num_user_ids, num_product_ids), dtype=np.float32)\n",
    "\n",
    "#         with open(filename, 'r') as train_file:\n",
    "#             matrix_reader = csv.reader(train_file, delimiter=',')\n",
    "#             next(matrix_reader, None)\n",
    "#             for row in matrix_reader:\n",
    "#                 dense_matrix[user_dict[row[0]], product_dict[row[1]]] = float(row[2])\n",
    "\n",
    "#         return dense_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_matrix_product_rows = merged_mtx_products('reviews.training.shortened.csv', user_dict, test_product_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_matrix_product_rows.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_matrix_product_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum(merged_matrix_product_rows[-1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_matrix_user_rows.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_matrix_user_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum(merged_matrix_user_rows[-1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Converts dense matrices to sparse.\n",
    "def to_sparse(filename_prefix, matrix):\n",
    "    print('Creating a sparse matrix...')\n",
    "    try:\n",
    "        # Try loading previously saved sparse matrix from file (becaues I keep crashing my kernel)\n",
    "        loader = np.load(filename_prefix + '.npz')\n",
    "        sparse_matrix = csr_matrix((loader['data'], loader['indices'], loader['indptr']), shape=loader['shape'], dtype=np.float32)\n",
    "        loader.close()\n",
    "    except:\n",
    "        # Create sparse matrix from dense matrix, write to file as backup\n",
    "        sparse_matrix = scipy.sparse.csr_matrix(matrix, dtype=np.float32)\n",
    "        scipy.sparse.save_npz((filename_prefix + 'npz'), sparse_matrix)\n",
    "    return sparse_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sparse_merged_matrix_pr = to_sparse('sparse.merged.matrix.pr', merged_matrix_product_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_matrix = to_sparse('merged.matrix', training_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sparse_merged_matrix_ur.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate global, row, and column means\n",
    "def calculate_means(sparse_matrix):\n",
    "    print('Calculating global mean...')\n",
    "    # global_mean = sparse_matrix.sum()/(sparse_matrix != 0).sum()\n",
    "    global_mean = np.true_divide(sparse_matrix.sum(), (sparse_matrix != 0).sum(), dtype=np.float32)\n",
    "    print(global_mean)\n",
    "    \n",
    "    print('Calculating row mean...')\n",
    "    matrix_row_mean = np.true_divide(sparse_matrix.sum(1), (sparse_matrix != 0).sum(1), dtype=np.float32)\n",
    "    print(matrix_row_mean[-1])\n",
    "    \n",
    "    np.savetxt(\"row.mean.csv\", matrix_row_mean, delimiter=\",\")\n",
    "    \n",
    "#     row_pad_len = len(test_user_dict) - len(user_dict)\n",
    "#     matrix_row_mean_padded = np.pad(matrix_row_mean, (0, row_pad_len), 'constant')\n",
    "#     print(matrix_row_mean_padded)\n",
    "    \n",
    "    \n",
    "    print('Calculating column mean...')\n",
    "    matrix_column_mean = np.true_divide(sparse_matrix.sum(0), (sparse_matrix != 0).sum(0), dtype=np.float32)\n",
    "    print(matrix_column_mean)\n",
    "    \n",
    "    np.savetxt(\"column.mean.csv\", matrix_column_mean.T, delimiter=\",\")\n",
    "\n",
    "    return global_mean, matrix_row_mean, matrix_column_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_mean, matrix_row_mean, matrix_column_mean = calculate_means(sparse_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge test data and normalize matrix.\n",
    "def normalize_matrix(sparse_matrix, global_mean, matrix_row_mean, matrix_column_mean):\n",
    "    \n",
    "    print('Creating a new matrix for merging test data scores...')\n",
    "\n",
    "    num_product_ids = len(test_product_dict)\n",
    "    num_user_ids = len(test_user_dict)\n",
    "\n",
    "    dense_merged_matrix = np.full((num_user_ids+1, num_product_ids+1), 0, dtype=np.float32)\n",
    "    \n",
    "    \n",
    "    with open('row.mean.csv', 'r') as infile:\n",
    "        file_reader = csv.reader(infile, delimiter=',')\n",
    "        row_count = 0\n",
    "        for row in file_reader:\n",
    "            dense_merged_matrix[row_count, :] = (float(row[0])-global_mean)/2\n",
    "            row_count += 1\n",
    "            \n",
    "    print(dense_merged_matrix)\n",
    "            \n",
    "    with open('column.mean.csv', 'r') as infile:\n",
    "        file_reader = csv.reader(infile, delimiter=',')\n",
    "        column_count = 0\n",
    "        for row in file_reader:\n",
    "            dense_merged_matrix[:, column_count] += (float(row[0])-global_mean)/2\n",
    "            column_count += 1\n",
    "\n",
    "#     print('Normalizing the data...')\n",
    "#     normalized_matrix = np.add(merged_matrix, matrix_row_mean_padded)\n",
    "    \n",
    "#     print('Normalized matrix: ')\n",
    "#     print(normalized_matrix.shape)\n",
    "#     print(normalized_matrix)\n",
    "\n",
    "    return dense_merged_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_matrix = normalize_matrix(sparse_matrix, global_mean, matrix_row_mean, matrix_column_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalized_matrix_users, matrix_row_mean_users, global_mean = normalize_merged_matrix(sparse_merged_matrix_ur, merged_matrix_user_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implements an SVD model.\n",
    "def compute_svd_from_normalized(normalized_matrix):\n",
    "\n",
    "    print('Computing svd from de-meaned matrix...')\n",
    "\n",
    "    U, sigma, Vt = svds(normalized_matrix, k = 100)\n",
    "    # U, sigma, Vt = np.linalg.svd(normalized_matrix)\n",
    "    S = np.diag(sigma)\n",
    "\n",
    "    return U, S, Vt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, S, Vt = compute_svd_from_normalized(normalized_matrix.astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_predictions = np.dot(np.dot(U, S), Vt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions_matrix = normalized_predictions+global_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_product_dict['B003F3NE1Q']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def query_matrix(infile, outfile, predictions_matrix):\n",
    "    \n",
    "    print(predictions_matrix.shape)\n",
    "    \n",
    "    with open(infile, 'r') as test_file:\n",
    "        test_reader = csv.reader(test_file, delimiter=',')\n",
    "        next(test_reader, None)\n",
    "        with open(outfile, 'w') as outfile:\n",
    "            outfile_reader = csv.writer(outfile, delimiter=',')\n",
    "            outfile_reader.writerow(['datapointID', 'overall'])\n",
    "\n",
    "            for row in test_reader:\n",
    "                \n",
    "                try:\n",
    "                    prediction = predictions_matrix[test_user_dict[row[0]], test_product_dict[row[1]]]\n",
    "                    outfile_reader.writerow([row[0], row[2], prediction])\n",
    "                    print(row[2], prediction)\n",
    "                except:\n",
    "                    print('Error')\n",
    "                    pass\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_matrix('reviews.dev.csv', 'reviews.test.labeled.csv', predictions_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Queries the prediction matrix.\n",
    "def query_normalized_matrix_test(test_file, prediction_file, test_user_dict, test_product_dict,\n",
    "                                predictions_product_rows, predictions_user_rows, global_mean):\n",
    "\n",
    "    print('Reconstructing matrix and making predictions...')\n",
    "\n",
    "    with open(test_file, 'r') as test_file:\n",
    "        test_reader = csv.reader(test_file, delimiter=',')\n",
    "        next(test_reader, None)\n",
    "        with open(prediction_file, 'w') as outfile:\n",
    "            outfile_reader = csv.writer(outfile, delimiter=',')\n",
    "            outfile_reader.writerow(['datapointID', 'overall'])\n",
    "            \n",
    "\n",
    "\n",
    "            for row in test_reader:\n",
    "                prediction = random.randrange(1,5,1)\n",
    "\n",
    "                try:\n",
    "                    # Query by user (new products on x axis).\n",
    "                    user_query = predictions_product_rows[test_user_dict[row[1]]]\n",
    "                    prediction = user_query[0, test_product_dict[row[2]]]\n",
    "                    outfile_reader.writerow([row[0], prediction])\n",
    "                    # print('Query by USER')\n",
    "                    # print(prediction)\n",
    "                except:\n",
    "                    pass\n",
    "#                     try:\n",
    "#                         # Query by product.\n",
    "#                         product_query = predictions_user_rows[product_dict[row[2]]]\n",
    "#                         prediction = product_query[0, test_user_dict[row[1]]]\n",
    "#                         outfile_reader.writerow([row[0], prediction])\n",
    "#                         # print('Query by PRODUCT')\n",
    "#                         # print(prediction)\n",
    "#                     except:\n",
    "#                         # If no matching users or products are found, make prediction based on global mean.\n",
    "#                         prediction = global_mean\n",
    "#                         outfile_reader.writerow([row[0], prediction])\n",
    "#                         # print('No matching query: GLOBAL MEAN')\n",
    "#                         # print(prediction)\n",
    "\n",
    "    print('Done.')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_ratings = query_normalized_matrix_test('reviews.dev.csv', 'reviews.test.labeled.csv',\n",
    "                                                test_user_dict, test_product_dict, predictions_product_rows,\n",
    "                                                predictions_user_rows, global_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# RMSE analysis\n",
    "def rmse(analysis_file):\n",
    "    \n",
    "    print('Checking RMSE...')\n",
    "    \n",
    "    targets = np.array([])\n",
    "    predictions = np.array([])\n",
    "\n",
    "    with open(analysis_file, 'r') as analysis_file:\n",
    "        analysis_reader = csv.reader(analysis_file, delimiter=',')\n",
    "        next(analysis_reader, None)\n",
    "        for row in analysis_reader:\n",
    "                targets = np.append(targets, row[1])\n",
    "                predictions = np.append(predictions, row[2])\n",
    "\n",
    "    return np.sqrt(((predictions - targets) ** 2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# rmse_val = rmse('reviews.dev.labeled.2.csv')\n",
    "# print(\"rms error is: \" + str(rmse_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
