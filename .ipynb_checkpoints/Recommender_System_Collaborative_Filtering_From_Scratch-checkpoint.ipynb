{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import gzip\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse import csc_matrix\n",
    "from scipy.sparse.linalg import svds\n",
    "import sklearn\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import normalize\n",
    "import csv\n",
    "from sparsesvd import sparsesvd\n",
    "import math\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unzip_json(filename):\n",
    "    \n",
    "    print('Unzipping json file...')\n",
    "    \n",
    "    unzipped_data = pd.read_json(gzip.open(filename))\n",
    "    \n",
    "    return unzipped_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Output json training data as a Pandas dataframe.\n",
    "def json_to_df(file_name):\n",
    "\n",
    "    print('Converting json file to dataframe...')\n",
    "\n",
    "    try:\n",
    "        training_data = pd.read_json(file_name, lines=True)\n",
    "        return training_data\n",
    "    except:\n",
    "        print('Please try another file name.')\n",
    "\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_to_csv(dataframe, desired_filename):\n",
    "\n",
    "    print('Converting dataframe to csv: ' + desired_filename + '...')\n",
    "\n",
    "    try:\n",
    "        return dataframe.to_csv(desired_filename, index=False)\n",
    "    except:\n",
    "        print('Please try another dataframe or file name.')\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Returns dictionaries with unique users and products as keys and unique ints as values.\n",
    "def create_user_product_dicts(filename):\n",
    "\n",
    "    print('Creating dictionaries from CSV for unique users and products...')\n",
    "\n",
    "    user_dict = {}\n",
    "    product_dict = {}\n",
    "    user_count = 0\n",
    "    product_count = 0\n",
    "\n",
    "    with open(filename, 'r') as train_file:\n",
    "        file_reader=csv.reader(train_file, delimiter=',')\n",
    "        next(file_reader, None)\n",
    "\n",
    "        for row in file_reader:\n",
    "            if row[0] not in user_dict:\n",
    "                user_dict[row[0]] = user_count\n",
    "                user_count += 1\n",
    "            if row[1] not in product_dict:\n",
    "                product_dict[row[1]] = product_count\n",
    "                product_count += 1\n",
    "\n",
    "    return user_dict, product_dict, user_count, product_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dictionaries from CSV for unique users and products...\n"
     ]
    }
   ],
   "source": [
    "user_dict, product_dict, user_count, product_count = create_user_product_dicts('reviews.training.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def training_mtx(filename, user_dict, product_dict):\n",
    "\n",
    "        print('Creating a dense matrix from training data...')\n",
    "\n",
    "        num_user_ids = len(user_dict)\n",
    "        num_product_ids = len(product_dict)\n",
    "\n",
    "        dense_matrix = np.zeros(shape=(num_user_ids, num_product_ids), dtype=np.float32)\n",
    "\n",
    "        with open(filename, 'r') as train_file:\n",
    "            matrix_reader = csv.reader(train_file, delimiter=',')\n",
    "            next(matrix_reader, None)\n",
    "            for row in matrix_reader:\n",
    "                dense_matrix[user_dict[row[0]], product_dict[row[1]]] = float(row[2])\n",
    "\n",
    "        return dense_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a dense matrix from training data...\n"
     ]
    }
   ],
   "source": [
    "training_matrix = training_mtx('reviews.training.csv', user_dict, product_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 5., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 5., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(123952, 50050)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Outputs dictionaries with unique test users and test products.\n",
    "def get_test_users_products(filename, training_user_dict, training_product_dict):\n",
    "\n",
    "    print('Importing test users and products...')\n",
    "\n",
    "    test_user_count = len(training_user_dict)\n",
    "    test_product_count = len(training_product_dict)\n",
    "    test_user_dict = training_user_dict.copy()\n",
    "    test_product_dict = training_product_dict.copy()\n",
    "\n",
    "    with open(filename, 'r') as test_file:\n",
    "        test_reader = csv.reader(test_file, delimiter=',')\n",
    "        next(test_reader, None)\n",
    "\n",
    "        for row in test_reader:\n",
    "            # Add unique users to test_user dictionary.\n",
    "            # print(row[1])\n",
    "            if row[1] not in test_user_dict:\n",
    "                test_user_dict[row[1]] = test_user_count\n",
    "                test_user_count += 1\n",
    "            # Add unique products to test_product dictionary.\n",
    "            # print(row[2])\n",
    "            if row[2] not in test_product_dict:\n",
    "                test_product_dict[row[2]] = test_product_count\n",
    "                test_product_count += 1\n",
    "\n",
    "    return test_user_dict, test_product_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing test users and products...\n"
     ]
    }
   ],
   "source": [
    "test_user_dict, test_product_dict = get_test_users_products('reviews.test.unlabeled.csv', user_dict, product_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(test_user_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Merging new users OR new products into the matrix along the x axis\n",
    "# def merged_mtx(test_file, train_file, test_y_axis, test_x_axis, train_y_axis,\n",
    "#                train_x_axis, test_y_axis_row_num, test_x_axis_row_num, train_y_axis_row_num, train_x_axis_row_num):\n",
    "\n",
    "#     print('Merging training and test data for ratings imputation...')\n",
    "\n",
    "#     num_user_ids = len(user_dict) # Training users only\n",
    "#     num_product_ids = len(test_x_axis) # Training and test products\n",
    "\n",
    "#     merged_matrix = np.zeros(shape=(num_user_ids, num_product_ids), dtype=np.float32)\n",
    "    \n",
    "#     print('Merged matrix shape: ')\n",
    "#     print(merged_matrix.shape)\n",
    "\n",
    "#     with open(test_file, 'r') as test_file:\n",
    "#         file_reader=csv.reader(test_file, delimiter=',')\n",
    "#         next(file_reader, None)\n",
    "\n",
    "#         for row in file_reader:\n",
    "            \n",
    "#             # print(row[1])\n",
    "#             # print(test_product_dict[row[2]])\n",
    "            \n",
    "#             merged_matrix[:, test_x_axis[row[test_x_axis_row_num]]] = float(0)\n",
    "\n",
    "#     with open(train_file, 'r') as train_file:\n",
    "#         file_reader=csv.reader(train_file, delimiter=',')\n",
    "#         next(file_reader, None)\n",
    "\n",
    "#         for row in file_reader:\n",
    "#             merged_matrix[train_y_axis[row[train_y_axis_row_num]], train_x_axis[row[train_x_axis_row_num]]] = float(row[2])\n",
    "\n",
    "#     return merged_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad the matrix to add extra products along the x axis\n",
    "\n",
    "def merged_mtx_products(filename, user_dict, test_product_dict):\n",
    "    \n",
    "        print('Creating a matrix with new products on the x axis...')\n",
    "\n",
    "        num_user_ids = len(user_dict)\n",
    "        num_product_ids = len(test_product_dict)\n",
    "\n",
    "        dense_matrix = np.zeros(shape=(num_user_ids, num_product_ids), dtype=np.float32)\n",
    "\n",
    "        with open(filename, 'r') as train_file:\n",
    "            matrix_reader = csv.reader(train_file, delimiter=',')\n",
    "            next(matrix_reader, None)\n",
    "            for row in matrix_reader:\n",
    "                dense_matrix[user_dict[row[0]], product_dict[row[1]]] = float(row[2])\n",
    "\n",
    "        return dense_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a matrix with new products on the x axis...\n"
     ]
    }
   ],
   "source": [
    "merged_matrix_product_rows = merged_mtx_products('reviews.training.csv', user_dict, test_product_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(123952, 51744)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_matrix_product_rows.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 5., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 5., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_matrix_product_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(merged_matrix_product_rows[-1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merged_mtx_users(filename, product_dict, test_user_dict):\n",
    "\n",
    "        print('Creating a matrix with new users on the x axis...')\n",
    "\n",
    "        num_product_ids = len(product_dict)\n",
    "        num_user_ids = len(test_user_dict)\n",
    "\n",
    "        dense_matrix = np.zeros(shape=(num_product_ids, num_user_ids), dtype=np.float32)\n",
    "\n",
    "        with open(filename, 'r') as train_file:\n",
    "            matrix_reader = csv.reader(train_file, delimiter=',')\n",
    "            next(matrix_reader, None)\n",
    "            for row in matrix_reader:\n",
    "                dense_matrix[product_dict[row[1]], user_dict[row[0]]] = float(row[2])\n",
    "\n",
    "        return dense_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a matrix with new users on the x axis...\n"
     ]
    }
   ],
   "source": [
    "merged_matrix_user_rows = merged_mtx_users('reviews.training.csv', product_dict, test_user_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50050, 123960)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_matrix_user_rows.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 5., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 5., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_matrix_user_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_sparse(filename_prefix, matrix):\n",
    "    print('Creating a sparse matrix...')\n",
    "    try:\n",
    "        # Try loading previously saved sparse matrix from file (becaues I keep crashing my kernel)\n",
    "        loader = np.load('sparse.merged.matrix.pr' + '.npz')\n",
    "        sparse_matrix = csr_matrix((loader['data'], loader['indices'], loader['indptr']), shape=loader['shape'])\n",
    "        loader.close()\n",
    "    except:\n",
    "        # Create sparse matrix from dense matrix, write to file as backup\n",
    "        sparse_matrix = scipy.sparse.csr_matrix(merged_matrix_product_rows)\n",
    "        scipy.sparse.save_npz((filename_prefix + 'npz'), sparse_merged_matrix_pr)\n",
    "    return sparse_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a sparse matrix...\n"
     ]
    }
   ],
   "source": [
    "sparse_merged_matrix_pr = to_sparse('sparse.merged.matrix.pr', merged_matrix_product_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(123952, 51744)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_merged_matrix_pr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a sparse matrix...\n"
     ]
    }
   ],
   "source": [
    "sparse_merged_matrix_ur = to_sparse('sparse.merged.matrix.ur', merged_matrix_user_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(123952, 51744)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_merged_matrix_ur.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_merged_matrix(sparse_matrix, dense_merged_matrix):\n",
    "\n",
    "    print('Calculating global mean...')\n",
    "    global_mean = sparse_matrix.sum()/(sparse_matrix != 0).sum()\n",
    "    print(global_mean)\n",
    "    \n",
    "    print('Calculating row mean...')\n",
    "    matrix_row_mean = sparse_matrix.sum(1)/(sparse_matrix != 0).sum(1)\n",
    "    # print(matrix_row_mean)\n",
    "\n",
    "    print('Normalizing the data...')\n",
    "    # count = 0\n",
    "    # normalized_matrix = np.zeros(shape=(len(dense_merged_matrix[:, 0]), len(dense_merged_matrix[0, :])))\n",
    "    normalized_matrix = sparse_matrix.copy()\n",
    "    normalized_matrix -= matrix_row_mean.reshape(-1, 1)\n",
    "    print(normalized_matrix.shape)\n",
    "\n",
    "#     # Seems kind of hacky, but normalizing in batches due to memory shortage\n",
    "#     # I should compress this to a function\n",
    "#     print('First normalization batch...')\n",
    "#     for row in range(0, 30000):\n",
    "#         try:\n",
    "#             # print(count)\n",
    "#             normalized_matrix[row, :] = sparse_matrix[row, :] - matrix_row_mean[row, :].reshape(-1, 1)\n",
    "#             # count += 1\n",
    "#         except:\n",
    "#             pass\n",
    "#     print('Second normalization batch...')\n",
    "#     for row in range(30000, 60000):\n",
    "#         try:\n",
    "#             # print(count)\n",
    "#             normalized_matrix[row, :] = sparse_matrix[row, :] - matrix_row_mean[row, :].reshape(-1, 1)\n",
    "#             # count += 1\n",
    "#         except:\n",
    "#             pass\n",
    "#     print('Third normalization batch...')\n",
    "#     for row in range(60000, 90000):\n",
    "#         try:\n",
    "#             # print(count)\n",
    "#             normalized_matrix[row, :] = sparse_matrix[row, :] - matrix_row_mean[row, :].reshape(-1, 1)\n",
    "#             # count += 1\n",
    "#         except:\n",
    "#             pass\n",
    "#     print('Fourth normalization batch...')\n",
    "#     for row in range(90000, 120000):\n",
    "#         try:\n",
    "#             # print(count)\n",
    "#             normalized_matrix[row, :] = sparse_matrix[row, :] - matrix_row_mean[row, :].reshape(-1, 1)\n",
    "#             # count += 1\n",
    "#         except:\n",
    "#             pass\n",
    "#     print('Fifth normalization batch...')\n",
    "#     for row in range(120000, row_mean.shape[0]-1):\n",
    "#         try:\n",
    "#             # print(count)\n",
    "#             normalized_matrix[row, :] = sparse_matrix[row, :] - matrix_row_mean[row, :].reshape(-1, 1)\n",
    "#             # count += 1\n",
    "#         except:\n",
    "#             pass\n",
    "    \n",
    "    print('Normalized matrix: ')\n",
    "    print(normalized_matrix.shape)\n",
    "    print(normalized_matrix)\n",
    "\n",
    "    return normalized_matrix, matrix_row_mean, global_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating global mean...\n",
      "4.110994929404886\n",
      "Calculating row mean...\n",
      "Normalizing the data...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "normalized_matrix_products, matrix_row_mean_products, global_mean = normalize_merged_matrix(sparse_merged_matrix_pr,\n",
    "                                                                                            merged_matrix_product_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "normalized_matrix_users, matrix_row_mean_users, global_mean = normalize_merged_matrix(sparse_merged_matrix_ur)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_svd_from_demeaned(urm_demeaned):\n",
    "\n",
    "    print('Computing svd from de-meaned matrix...')\n",
    "\n",
    "    U, sigma, Vt = svds(urm_demeaned, k = 100)\n",
    "    S = np.diag(sigma)\n",
    "\n",
    "    return U, S, Vt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U_products, S_products, Vt_products = compute_svd_from_demeaned(normalized_matrix_products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U_users, S_users, Vt_users = compute_svd_from_demeaned(normalized_matrix_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_matrix_products.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(np.dot(U_products, S_products), Vt_products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# U_products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S_products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Vt_products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_matrix_users.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(np.dot(U_users, S_users), Vt_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# U_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# S_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Vt_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_demeaned_matrix(test_file, prediction_file, test_user_dict, test_product_dict,\n",
    "                                U_products, S_products, Vt_products, U_users, S_users, Vt_users,\n",
    "                                matrix_row_mean_products, matrix_row_mean_users, global_mean):\n",
    "\n",
    "    print('Reconstructing matrix and making predictions...')\n",
    "    \n",
    "#     print(len(test_user_dict))\n",
    "#     print(len(test_product_dict))\n",
    "    \n",
    "    \n",
    "    right_term_products = np.dot(S_products, Vt_products)\n",
    "    right_term_users = np.dot(S_users, Vt_users)\n",
    "\n",
    "    with open(test_file, 'r') as test_file:\n",
    "        test_reader = csv.reader(test_file, delimiter=',')\n",
    "        next(test_reader, None)\n",
    "        with open(prediction_file, 'w') as outfile:\n",
    "            outfile_reader = csv.writer(outfile, delimiter=',')\n",
    "            outfile_reader.writerow(['datapointID', 'overall'])\n",
    "\n",
    "            for row in test_reader:\n",
    "\n",
    "                try: \n",
    "                    # Query by user.\n",
    "                    user_query = np.dot(right_term_products.T, U_products[test_user_dict[row[0]], :].T)\n",
    "                    prediction = -user_query[test_product_dict[row[1]]]\n",
    "                    outfile_reader.writerow([row[0], prediction])\n",
    "#                     print('Query by USER')\n",
    "#                     print(row[1])\n",
    "#                     print(prediction)\n",
    "                except:\n",
    "                    try:\n",
    "                        # Query by product.\n",
    "                        product_query = np.dot(right_term_users.T, U_users[test_product_dict[row[1]], :].T)\n",
    "                        prediction = -product_query[test_user_dict[row[0]]]\n",
    "                        outfile_reader.writerow([row[0], prediction])\n",
    "#                         print('Query by PRODUCT')\n",
    "#                         print(row[1])\n",
    "#                         print(prediction)\n",
    "                    except:\n",
    "                        # If no matching users or products are found, make prediction based on global mean.\n",
    "                        prediction = global_mean\n",
    "                        outfile_reader.writerow([row[0], prediction])\n",
    "#                         print('No matching query: GLOBAL MEAN')\n",
    "#                         print(row[1])\n",
    "#                         print(prediction)\n",
    "\n",
    "    print('Done.')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_ratings = reconstruct_demeaned_matrix('reviews.test.unlabeled.csv', 'reviews.test.labeled.csv',\n",
    "                                                test_user_dict, test_product_dict,\n",
    "                                                U_products, S_products, Vt_products, U_users, S_users, Vt_users, \n",
    "                                                matrix_row_mean_products, matrix_row_mean_users,\n",
    "                                                global_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
